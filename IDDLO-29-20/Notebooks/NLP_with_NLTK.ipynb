{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data: Corpora, Models and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download() # run one single time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "data_path = os.path.join(\"datasets\", \"\")\n",
    "download_path = \"https://raw.githubusercontent.com/AbdelMahm/FSR/master/IDDLO-29-20/Notebooks/datasets/\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "for filename in (\"text_ar.txt\", \"text_fr.txt\"):\n",
    "    print(\"Downloading\", filename)\n",
    "    url = download_path + filename\n",
    "    urllib.request.urlretrieve(url, data_path + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "data_path = os.path.join(\"datasets\", \"\")\n",
    "\n",
    "file_fr = open(data_path + 'text_fr.txt', 'r')\n",
    "file_ar = open(data_path + 'text_ar.txt', 'r')\n",
    "\n",
    "print(file_fr)\n",
    "print(file_ar)\n",
    "print('\\n')\n",
    "\n",
    "text_fr = file_fr.read()\n",
    "text_ar = file_ar.read()\n",
    "\n",
    "print(text_fr)\n",
    "print('\\n')\n",
    "print(text_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System default encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import textcat\n",
    "\n",
    "cls = textcat.TextCat()\n",
    "distances = cls.lang_dists(text_fr)\n",
    "cls.guess_language(text_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arabic Reshaping for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import arabic_reshaper\n",
    "#from bidi.algorithm import get_display\n",
    " \n",
    "#reshaped_text = arabic_reshaper.reshape(text_ar)\n",
    "#bidi_text = get_display(reshaped_text)\n",
    "#print(bidi_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Numbers using re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "numbers = re.findall(r'[0-9]+', text_ar) \n",
    "print(numbers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Numbers using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[0-9]+')\n",
    "numbers=tokenizer.tokenize(text_ar)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dates\n",
    "dates_text = \"\"\"5-2-2020, 15/2/2020, 2020/2/4 autre autre\"\"\"\n",
    "dates = re.findall(r'(\\d{1,4}[.\\-/]\\d{1,2}[.\\-/]\\d{1,4})', dates_text) \n",
    "print(dates) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email\n",
    "email_text = \"\"\"ahmed@dgi.gov.ma, maryam@dgi.ma ahmadi3maryam@gmail.com other text here\"\"\"\n",
    "emails = re.findall(r'[\\w.-]+@[\\w.-]+', email_text) \n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text_fr))\n",
    "print('\\n')\n",
    "print(word_tokenize(text_ar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this tokenizer is traind on more data\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "print(wordpunct_tokenize(text_ar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(text_fr))\n",
    "print('\\n')\n",
    "print(sent_tokenize(text_ar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS (Part-Of-Speech) Tagging & Chunking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "text = text_fr.split()\n",
    "print(\"After Split:\",text)\n",
    "print(\"\\n\")\n",
    "pos_tagged = pos_tag(text)\n",
    "print(\"After Token:\",pos_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing using a Chunk Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpParser\n",
    "\n",
    "patterns= \"\"\"mychunk:{<NNP.?>*<CD.?>}\"\"\"\n",
    "chunk_parser = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunk_parser)\n",
    "output = chunk_parser.parse(pos_tagged)\n",
    "print(\"After chunk parsing\",output)\n",
    "output.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokens_fr = nltk.word_tokenize(text_fr)\n",
    "tokens_ar = nltk.word_tokenize(text_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### French Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "print (SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=SnowballStemmer(\"french\", ignore_stopwords=True)\n",
    "for w in tokens_fr:\n",
    "    print(\"{} ===> {}\".format(w,stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=SnowballStemmer(\"arabic\", ignore_stopwords=True)\n",
    "for w in tokens_ar:\n",
    "    print(\"{} ===> {}\".format(w,stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using FrenchStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "stemmer  = FrenchStemmer()\n",
    "for w in tokens_fr:\n",
    "    print(\"{} ===> {}\".format(w,stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write stems in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_stem_file=open(\"text_fr_stems.txt\",mode=\"a+\")\n",
    "fr_stem_file.truncate(0)\n",
    "stem_sentence = []\n",
    "for w in tokens_fr:\n",
    "    stem_sentence.append(stemmer.stem(w))\n",
    "    stem_sentence.append(\" \")\n",
    "stem_sentence = \"\".join(stem_sentence)        \n",
    "fr_stem_file.write(stem_sentence)\n",
    "\n",
    "fr_stem_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arabic Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ArabicStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import ArabicStemmer\n",
    "\n",
    "stemmer  = ArabicStemmer()\n",
    "\n",
    "for w in tokens_ar:\n",
    "    print(\"{} ===> {}\".format(w,stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arabic ARLSTem (More Recent 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.arlstem import ARLSTem\n",
    "\n",
    "stemmer  = ARLSTem()\n",
    "\n",
    "for w in tokens_ar:\n",
    "    print(\"{} ===> {}\".format(w,stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for w in tokens_fr:\n",
    "    print(\"{} ===> {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"S -> NP VP\n",
    "VP -> V NP\n",
    "NP -> 'Ahmed' | 'book'\n",
    "V -> 'reads'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(\"Ahmed reads book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "trees = parser.parse_all(text)\n",
    "\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsers for other langauges\n",
    "#!brew install stanford-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "tree = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find RC in Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "charika_df = pd.read_excel(data_path + 'BD_IDENTIFICATION_ETABLISSEMENTS_PRIVES_VF.xlsx', sheet_name='BD_CHARIKA', index_col=0)\n",
    "\n",
    "charika_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = wordpunct_tokenize(text_ar)\n",
    "\n",
    "RC_num = int(tokens[tokens.index('RC') - 2])\n",
    "\n",
    "print(RC_num)\n",
    "\n",
    "charika_df.index[charika_df['RC_CHARIKA'] == RC_num]\n",
    "\n",
    "#charika_df.index[charika_df['RC_CHARIKA'] == RC_num].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "sent1 = \"It might help to re-install Python if possible.\"\n",
    "sent2 = \"It can help to install Python again if possible.\"\n",
    " \n",
    "nltk.edit_distance(sent1, sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "text = text_fr.split()\n",
    "print(\"After Split:\",text)\n",
    "print(\"\\n\")\n",
    "pos_tagged = pos_tag(text)\n",
    "print(\"After Token:\",pos_tagged)\n",
    "\n",
    "ne = nltk.ne_chunk(pos_tagged)\n",
    "ne.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "# French CLTK Corpora\n",
    "corpus_importer = CorpusImporter('french')\n",
    "corpus_importer.list_corpora\n",
    "corpus_importer.import_corpus('french_data_cltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arabic CLTK Corpora\n",
    "#corpus_importer = CorpusImporter('arabic')\n",
    "#corpus_importer.list_corpora\n",
    "#corpus_importer.import_corpus('arabic_text_perseus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tag.ner import NamedEntityReplacer\n",
    "\n",
    "text_str = \"\"\"La France a célébré Vendredi dernier sa fête d'indépendance. François Sarkozy a prononcé son discours.\"\"\"\n",
    "\n",
    "ner_replacer = NamedEntityReplacer()\n",
    "\n",
    "ner_replacer.tag_ner_fr(text_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
